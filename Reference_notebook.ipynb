{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "# For numeric/scientific calculations\n",
    "import numpy as np\n",
    "# For Data Reading and preprocessing\n",
    "import pandas as pd\n",
    "# For Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# For Machine learning packacges we use scikit-lean. It will be shown when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file\n",
    "adult_df = pd.read_csv(\"adult.csv\")\n",
    "comp_df = pd.read_csv(\"computers.csv\")\n",
    "cars_df = pd.read_csv(\"Cars93.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data sets\n",
    "print('Shape of adult dataframe:',adult_df.shape)\n",
    "print('*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
    "print('Shape of cars93 dataframe:',cars_df.shape)\n",
    "print('*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
    "print('Shape of computers dataframe:',comp_df.shape)\n",
    "print('*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data frames:\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering information about the columns:\n",
    "adult_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kindly note that any string data type will be mentioned as object and not as string\n",
    "# int 32 and int 64 have different item size. This holds good for other data types as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding unknown values present in adult dataset: '?' to 'NaN'\n",
    "adult_df[adult_df == '?'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether changes are made:\n",
    "adult_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see that there are null object now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['workclass'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the values we can be sure that work class is a categorical variable\n",
    "# thus to fill in null values, it is more feasible to use the central tendency: 'mode'\n",
    "adult_df['workclass'].fillna(adult_df['workclass'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether changes made are effective:\n",
    "adult_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for occupation and country:\n",
    "adult_df['occupation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['occupation'].fillna(adult_df['occupation'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['native.country'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df['native.country'].fillna(adult_df['native.country'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus all null objects are removed\n",
    "# seeing the columns and deciding the target variable\n",
    "adult_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be inferred that income can be selected as a target variable or dependent variable (y)\n",
    "# the othe columns are treated as independent variables (X)\n",
    "# Splitting the dataset into independent and dependent variables is done as shown below\n",
    "X = adult_df.drop(['income'], axis=1)\n",
    "y = adult_df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up of the data set into test data and traning data:\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We shall split the dataset into 2 parts: 80% - training data and 20% - test data\n",
    "# thus test size is 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering:\n",
    "# Based on column names, we can determine which are the categorical variables. \n",
    "# But we have also seen that those columns have values in terms of string data type.\n",
    "# thus it is more feasible to encode those values into numbers.\n",
    "# we shall also scale those numbers so as to obtain \n",
    "from sklearn import preprocessing\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', \n",
    "               'race', 'sex', 'native.country']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.fit_transform(X_test[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling:\n",
    "# u can use min max scalar or standard scalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the changes made:\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for PCA (Principal component analysis)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train = pca.fit_transform(X_train)\n",
    "pca.explained_variance_ratio_\n",
    "# explained_variance_ratio_ => indicates the proportion \n",
    "# of the datasetâ€™s variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall now move on to regression\n",
    "# Fir, let us see the data set:\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "plt.scatter(comp_df['Units'], comp_df['Minutes'])\n",
    "plt.xlabel(\"Number of Units\")\n",
    "plt.ylabel(\"Time to repair in Minutes\")\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upon observation of the plot, we can be sure that the association is linear\n",
    "# we shall check for the linear relation coefficient to be sure\n",
    "comp_df.corr(method  = \"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe that there exists a high correlation between the two variables considered.\n",
    "# Thus we can proceed with linear model\n",
    "# Do note that the mean or median is not the right way of summarising for the given data.\n",
    "# It is more feasible to build a model for the same.\n",
    "# You can observe in the graph below for justification:\n",
    "comp_mean = comp_df.Minutes.mean()\n",
    "comp_df['MeanTime'] = comp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x=\"Units\",y=\"Minutes\",data=comp_df)\n",
    "ax.add_line(plt.Line2D(comp_df.Units,comp_df.MeanTime,color=\"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating error based on mean model:\n",
    "comp_error_df = pd.DataFrame(np.array([comp_df.Units,\n",
    "              comp_df.Minutes,\n",
    "              comp_df.MeanTime,\n",
    "              comp_df.MeanTime - comp_df.Minutes]).T,\n",
    "              columns=[\"Units\", \"Actual time\", \"Predicted time\", \"Error\"])\n",
    "comp_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_error_df['Error'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we observe that the summation of errors is close to zero, \n",
    "# thus it is more feasible to have an error in the form of higher power. \n",
    "# Hence we use Mean squared error, RMSE, etc.\n",
    "\n",
    "comp_error_df = pd.DataFrame(np.array([comp_df.Units,\n",
    "              comp_df.Minutes,\n",
    "              comp_df.MeanTime,\n",
    "              comp_df.MeanTime - comp_df.Minutes,\n",
    "              (comp_df.MeanTime - comp_df.Minutes)**2]).T,\n",
    "              columns=[\"Units\", \"Actual time\", \"Predicted time\", \"Error\", \"Sq.Error\"])\n",
    "comp_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(comp_error_df['Sq.Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our aim is to bring this sum of squared errors to as low value as possible.\n",
    "# This is obtained by applying a regression model\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_model = LinearRegression()\n",
    "comp_model.fit(X = comp_df.loc[:,[\"Units\"]], y= comp_df.loc[:,[\"Minutes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = comp_model.intercept_+comp_model.coef_[0,0]*(comp_df['Units'])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df_final = pd.DataFrame(np.array([comp_df.Units,\n",
    "                                      comp_df.Minutes,\n",
    "                                      y_pred,\n",
    "                                      comp_df.Minutes - y_pred,\n",
    "                                      (comp_df.Minutes - y_pred)**2]).T,\n",
    "                            columns = [\"Units\", \"Actual Time\", \"Predicted Time\", \"Error\", \"Sq.Error\"])\n",
    "comp_df_final                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(comp_df_final['Sq.Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x=\"Units\",y=\"Minutes\",data=comp_df)\n",
    "ax.add_line(plt.Line2D(comp_df.Units,comp_df.MeanTime,color=\"red\"))\n",
    "ax.add_line(plt.Line2D(comp_df.Units,comp_df_final['Predicted Time'],color=\"black\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating our model\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(comp_df[[\"Units\"]])\n",
    "y = comp_df[\"Minutes\"]\n",
    "model = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_model.score(X = np.array(comp_df['Units']).reshape(-1,1), y = comp_df['Minutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can observe that the model score is same as r-squared value\n",
    "# Adjusted r-square is more accurate in case of multi linear regression\n",
    "# any r-squared or score will tell you how accurate your model will be. in the given example,\n",
    "# it can be inferred from the score that the value predicted \n",
    "# will be as close as 98.7% to the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
